{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\amrapali\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense, Activation, Dropout, UpSampling2D, RepeatVector,TimeDistributed\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten,BatchNormalization,concatenate,LSTM\n",
    "from keras.layers import Input, Lambda,Reshape,Multiply, add, Add\n",
    "from keras.models import Model\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json\n",
    "from gensim.models import KeyedVectors\n",
    "import keras.backend as K\n",
    "from numpy import prod\n",
    "from nltk.corpus import wordnet as wn\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imageData():\n",
    "    with open('./data/img_features.json', 'r') as f:\n",
    "        feat = json.load(f) \n",
    "    return feat\n",
    "# assumption: imageid at end of question. \n",
    "# later from start in enco-decoder remove all tags, 'the', ',' (in answers), keep _ words \n",
    "# correct imageids not starting with i\n",
    "def load_QA(filename):\n",
    "    data = open(filename)\n",
    "    questList,ansList,imageIds = [],[],[]\n",
    "    for line in data.readlines():\n",
    "        line = line.replace(\"?\\n\",\"?\") \n",
    "        line = line.replace(\".\",\" . \")   \n",
    "        line = line.replace(\", \",\",\")\n",
    "        line = line.replace(\"_\",\",\")\n",
    "        #line = line.replace(\"the\",\"\")\n",
    "        if not (line.endswith(\"?\")):\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            line = \"<start>\"+\",\"+line\n",
    "            ansList.append(line.split(\",\"))\n",
    "        else:\n",
    "            imgId = line.split(\" \")[-2]\n",
    "            if imgId.startswith(\"i\"):\n",
    "                imageIds.append(imgId)\n",
    "            else:\n",
    "                print(\"incorrect image ID\" + line)\n",
    "                i = re.search(\"\\d\", imgId)\n",
    "                imgId = \"image\"+imgId[i.start():]\n",
    "                imageIds.append(imgId)\n",
    "            line = re.sub(r\"image\\d+\", \"\", line)\n",
    "            line = line.replace(\"  \",\" \")\n",
    "            temp = line.split(\" \")[0:-3]\n",
    "            temp.append(\"?\")\n",
    "            #questList.append(line.split(\" \")[0:-3])\n",
    "            questList.append(temp)\n",
    "    return (questList,ansList,imageIds)\n",
    "\n",
    "def create_vocab_df(vocab):\n",
    "    vocab_df = pd.DataFrame(list(vocab))\n",
    "    vocab_df.columns = [\"vocab\"]\n",
    "    vocab_df[\"index\"]=vocab_df.index\n",
    "    wordVecList = []\n",
    "    for i,word in enumerate(vocab_df[\"vocab\"]):\n",
    "        wordVecList.append(wv[word])\n",
    "    vocab_df[\"wordVec\"] = wordVecList\n",
    "    return vocab_df\n",
    "\n",
    "def max_len(data):\n",
    "    maxLen = 0\n",
    "    for i,u in enumerate(data):\n",
    "        if maxLen < np.asarray(u).shape[0]:\n",
    "            maxLen = np.asarray(u).shape[0]  \n",
    "    return maxLen\n",
    "\n",
    "def save_model(model,model_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save_weights(model_name+\".h5\")\n",
    "    \n",
    "def load_model(savedModel_filename):\n",
    "    filename = savedModel_filename+\".json\"\n",
    "    json_file = open(filename, 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    filename = savedModel_filename+\".h5\"\n",
    "    loaded_model.load_weights(filename)\n",
    "    return loaded_model\n",
    "\n",
    "def load_wordEmbeddings(wv_filename):\n",
    "    wv = KeyedVectors.load(wv_filename,mmap='r')\n",
    "    wv.init_sims(replace=True)\n",
    "    return wv\n",
    "\n",
    "def get_vocab(dataList):\n",
    "    vocab = set()\n",
    "    for aList in dataList:\n",
    "        for a in aList:\n",
    "            vocab.add(a)\n",
    "    vocab.add(\"<Pad>\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading encoder-decoder model \n",
    "loaded_model = load_model('./saved_models/modelTFiDFWeights-2')\n",
    "# load questions, answers and imageids\n",
    "questData, ansData, imageIds = load_QA(\"./data/Text-Q&A-train.txt\")\n",
    "testValQuestData, testValAnsData, valImageIds = load_QA(\"./data/Text-Q&A-test.txt\")\n",
    "#create a dataframe to store vocabulary, it's ref index and word embeddings\n",
    "wv = load_wordEmbeddings(\"./saved_models/word2vec-train1.kv\")\n",
    "vocab_df = create_vocab_df(wv.vocab.keys())\n",
    "# get max question and answer length for padding\n",
    "maxQuestLen, maxAnsLen = max_len(questData), max_len(ansData)\n",
    "# initialize variables\n",
    "batchSize = 50\n",
    "epoch = 50\n",
    "vecSize = wv.vector_size\n",
    "vocabSize = vocab_df.shape[0]\n",
    "#import image feat\n",
    "feat = load_imageData()\n",
    "#load model =>use old wv=>start tag is <start>, vocab has 1002 words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansVocab = get_vocab(ansData)\n",
    "ansVocab = sorted(ansVocab)\n",
    "ansVocabdf = create_vocab_df(ansVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual QA model architecture\n",
    "imgInput = Input(shape=(np.array(feat['image1']).shape), dtype='float32', name='images')\n",
    "input_quest = loaded_model.get_input_at(0)\n",
    "#img = Conv2D(512,(2,2),padding = 'valid',activation = 'relu')(imgInput)\n",
    "img = MaxPooling2D(pool_size=(2,2),padding='same')(imgInput)\n",
    "img = BatchNormalization()(img)\n",
    "img = Conv2D(256,(1,1),padding = 'same',activation = 'relu')(img)\n",
    "img = Conv2D(256,(2,2),padding = 'valid',activation = 'relu')(img)\n",
    "img = Conv2D(256,(2,2),padding ='same',activation = 'relu')(img)\n",
    "img = Conv2D(128,(1,1),padding ='same',activation = 'relu')(img)\n",
    "#img = Conv2D(256,(2,2),activation = 'relu',padding ='valid')(img)\n",
    "#img = Conv2D(128,(1,1),activation = 'relu',padding ='valid')(img) # batchsize,6,6,128\n",
    "#img = Conv2D(64,(1,1),activation = 'relu',padding ='same')(img)\n",
    "#img = Conv2D(1,(1,1),padding ='same')(img)\n",
    "summaryQuest = loaded_model.get_layer(\"bidirectional_1\").output #batchSize,128\n",
    "group0_a = Lambda(lambda x: x[:,0:3,0:3,:], output_shape=(3,3,128))(img) # batchSize,3,3,128\n",
    "group1_a = Lambda(lambda x: x[:,0:3,3:6,:], output_shape=(3,3,128))(img)\n",
    "group2_a = Lambda(lambda x: x[:,3:6,0:3,:], output_shape=(3,3,128))(img)\n",
    "group3_a = Lambda(lambda x: x[:,3:6,3:6,:], output_shape=(3,3,128))(img)\n",
    "\n",
    "group0_a = Flatten()(group0_a) # batchSize,1152\n",
    "concat0 = concatenate([group0_a,summaryQuest], axis = 1) #batchSize,(1152+128)\n",
    "concat01 = Reshape(target_shape=(1,1280))(concat0)  #batchSize,1,(1152+128)\n",
    "group1_a = Flatten()(group1_a)\n",
    "concat1 = concatenate([group1_a,summaryQuest], axis = 1)\n",
    "concat11 = Reshape(target_shape=(1,1280))(concat1)\n",
    "group2_a = Flatten()(group2_a)\n",
    "concat2 = concatenate([group2_a,summaryQuest], axis = 1)\n",
    "concat21 = Reshape(target_shape=(1,1280))(concat2)\n",
    "group3_a = Flatten()(group3_a)\n",
    "concat3 = concatenate([group3_a,summaryQuest], axis = 1)\n",
    "concat31 = Reshape(target_shape=(1,1280))(concat3)\n",
    "\n",
    "#answers = Input(shape=(maxAnsLen,vecSize), dtype='float32', name='answers') #batchsize,maxAnsLen,100\n",
    "\n",
    "inputConcat = concatenate([concat01,concat11,concat21,concat31], axis = 1) # batchsize,2,2380\n",
    "#inputConcat = concatenate([inputConcat,concat21], axis = 1) # batchsize,3,2380\n",
    "#inputConcat = concatenate([inputConcat,concat31], axis = 1) # batchsize,4,2380\n",
    "inputConcat = BatchNormalization()(inputConcat)\n",
    "lstm_out, hidden_state, cell_state = LSTM(512,return_sequences = True,return_state=True)(inputConcat)  #batchSize,\n",
    " \n",
    "inputFinal = BatchNormalization()(hidden_state)\n",
    "inputFinal = RepeatVector(maxAnsLen)(inputFinal)                 #output_shape: (batchSize,maxAnsLen,512)\n",
    "#inputFinal = concatenate([inputFinal,answers], axis = 2)  #input length 128+100, shape = batchSize,16,228\n",
    "\n",
    "lstm_out2, hidden_state2, cell_state2 = LSTM(128,return_sequences = True,return_state=True)(inputFinal)\n",
    "# kernel_regularizer=regularizers.l2(0.001)\n",
    "output3 = (TimeDistributed(Dense(len(ansVocab), activation=\"softmax\")))(lstm_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrapali\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ti..., inputs=[<tf.Tenso...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "modelQA = Model(input=[imgInput,input_quest], outputs=output3)\n",
    "modelQA.compile(loss=\"categorical_crossentropy\", optimizer='adam',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelQA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encInp(batchData,maxLen):\n",
    "    encInp = np.ones((len(batchData),maxLen,vecSize))*(np.asarray(vocab_df[\"wordVec\"].loc[vocab_df[\"vocab\"]==\"<Pad>\"])[0])\n",
    "    for i,wordList in enumerate(batchData):\n",
    "        for w,word in enumerate(wordList):\n",
    "            if word in wv.vocab:\n",
    "                encInp[i][w] = np.asarray(vocab_df[\"wordVec\"].loc[vocab_df['vocab'] ==word])[0]\n",
    "            else:\n",
    "                encInp[i][w] = np.asarray(vocab_df[\"wordVec\"].loc[vocab_df['vocab'] ==\"<Unk>\"])[0]\n",
    "    return encInp\n",
    "            \n",
    "def get_imgInp(batchData,batchImageIds):\n",
    "    imgInp = np.zeros((len(batchData),np.array(feat['image1']).shape[0],np.array(feat['image1']).shape[1],np.array(feat['image1']).shape[2]))\n",
    "    for i,imgId in enumerate(batchImageIds):\n",
    "        imgInp[i] = feat[imgId] \n",
    "    maxV = np.max(imgInp)\n",
    "    imgInp= imgInp.astype(\"float32\")/(float)(maxV)\n",
    "    return imgInp \n",
    "\n",
    "def get_oneHotTarget(batchData,vocabSize,vocab_df): \n",
    "    target = np.zeros((len(batchData),maxAnsLen,vocabSize))\n",
    "    target[:,:,vocab_df[\"index\"].loc[vocab_df[\"vocab\"]== \"<Pad>\"]] = 1\n",
    "    for i,questList in enumerate(batchData):\n",
    "        for w,quest in enumerate(questList[1:]):\n",
    "            if quest in list(vocab_df[\"vocab\"]):\n",
    "                target[i,w,vocab_df[\"index\"].loc[vocab_df[\"vocab\"]== \"<Pad>\"]] = 0\n",
    "                target[i,w,vocab_df[\"index\"].loc[vocab_df[\"vocab\"]== quest]] = 1\n",
    "    return target\n",
    "\n",
    "def calculate_class_weights(trainTarget):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(trainTarget)[2]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for nd in range(number_dim):\n",
    "        y_true = [0]*len(trainTarget)\n",
    "        for i in list(np.where(trainTarget[:,:,nd] ==1)[0]):\n",
    "            y_true[i] =1\n",
    "        weights[nd] = compute_class_weight('balanced', np.unique(y_true), y_true)\n",
    "    return weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "batchSize = 100\n",
    "valBatchData = testValQuestData[0:50]  # 50 samples for validation \n",
    "valBatchDataOp = testValAnsData[0:50]\n",
    "valBatchImgId = valImageIds[0:50]\n",
    "valEncInp = get_encInp(valBatchData,maxQuestLen)\n",
    "valTarget = get_oneHotTarget(valBatchDataOp,len(ansVocab),ansVocabdf)\n",
    "valImg = get_imgInp(valBatchData,valBatchImgId)\n",
    "for e in range(100):\n",
    "    print(\"training epoch \"+str(e)) \n",
    "    for itr in range(int(math.ceil(np.asarray(questData).shape[0]/batchSize))):\n",
    "        if (np.asarray(questData).shape[0] - (itr*batchSize) >= batchSize):\n",
    "            startIx = (itr*batchSize)%(np.asarray(questData).shape[0])\n",
    "            #startIx = 0\n",
    "        else:\n",
    "            break\n",
    "        # prepare batch data\n",
    "        batchData = questData[startIx:startIx+batchSize] \n",
    "        batchDataOp = ansData[startIx:startIx+batchSize]\n",
    "        batchImgId = imageIds[startIx:startIx+batchSize]\n",
    "        #encoder Inputs\n",
    "        trainEncInp = get_encInp(batchData,maxQuestLen)\n",
    "        #image input\n",
    "        trainImg = get_imgInp(batchData,batchImgId)\n",
    "        #answer targets\n",
    "        trainTarget = get_oneHotTarget(batchDataOp,len(ansVocab),ansVocabdf)\n",
    "        #class weights\n",
    "        class_weights = calculate_class_weights(trainTarget)\n",
    "        # training the model.             \n",
    "        modelQA.fit([trainImg,trainEncInp], \n",
    "                    trainTarget,epochs=1,shuffle=True,batch_size=batchSize,class_weight=class_weights)\n",
    "#validation_data=([valImg,valEncInp],valTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results on test data\n",
    "batchData = testValQuestData[0:100]\n",
    "batchDataOp = testValAnsData[0:100]\n",
    "batchImgId = valImageIds[0:100]\n",
    "EncInp = get_encInp(batchData,maxQuestLen)\n",
    "trainImg = get_imgInp(batchData,batchImgId)\n",
    "modelPred = modelQA.predict([trainImg,EncInp])\n",
    "target = multiClass_target(ansVocab,ansVocabdf,batchDataOp)\n",
    "PredTarget = multiClass_pred(ansVocab,ansVocabdf,modelPred)\n",
    "precision,recall = metric_check(PredTarget,target)\n",
    "actualAnsList,PredAnsList = get_ActandPred_list(modelPred,ansVocabdf,batchDataOp)\n",
    "calculate_WUP(actualAnsList,PredAnsList)\n",
    "print(\"Precision:\"+str(precision))\n",
    "print(\"Recall\"+str(recall))\n",
    "print_predictions(modelPred,batchData,batchDataOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-211b3aac303f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mEncInp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_encInp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxQuestLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrainImg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_imgInp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchImgId\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodelPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelQA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainImg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEncInp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiClass_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mansVocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mansVocabdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchDataOp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mPredTarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiClass_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mansVocab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mansVocabdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelPred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1515\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         return self._predict_loop(f, ins,\n\u001b[1;32m-> 1517\u001b[1;33m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m   1518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1519\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1141\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1142\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# results on train data\n",
    "batchData = questData[30:50]\n",
    "batchDataOp = ansData[30:50]\n",
    "batchImgId = imageIds[30:50]\n",
    "EncInp = get_encInp(batchData,maxQuestLen)\n",
    "trainImg = get_imgInp(batchData,batchImgId)\n",
    "modelPred = modelQA.predict([trainImg,EncInp])\n",
    "target = multiClass_target(ansVocab,ansVocabdf,batchDataOp)\n",
    "PredTarget = multiClass_pred(ansVocab,ansVocabdf,modelPred)\n",
    "precision,recall = metric_check(PredTarget,target)\n",
    "actualAnsList,PredAnsList = get_ActandPred_list(modelPred,ansVocabdf,batchDataOp)\n",
    "calculate_WUP(actualAnsList,PredAnsList)\n",
    "print(\"Precision:\"+str(precision))\n",
    "print(\"Recall\"+str(recall))\n",
    "print_predictions(modelPred,batchData,batchDataOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train to overfit\n",
    "batchData = questData[0:100] \n",
    "batchDataOp = ansData[0:100]\n",
    "batchImgId = imageIds[0:100]\n",
    "trainEncInp = get_encInp(batchData,maxQuestLen)\n",
    "#answers inputs\n",
    "#trainAnswerIp= get_encInp(batchDataOp,maxAnsLen)\n",
    "#image input\n",
    "trainImg = get_imgInp(batchData,batchImgId)\n",
    "#answer targets\n",
    "trainTarget = get_oneHotTarget(batchDataOp,len(ansVocab),ansVocabdf)\n",
    "class_weights = calculate_class_weights(trainTarget)\n",
    "# training the model. \n",
    "for e in range(80):\n",
    "    print(str(e))\n",
    "    modelQA.fit([trainImg,trainEncInp],trainTarget,epochs=1,\n",
    "                shuffle=True,batch_size=50,class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print predictions\n",
    "# batchData = questData[0:10]\n",
    "# batchDataOp = ansData[0:10]\n",
    "# batchImgId = imageIds[0:10]\n",
    "# EncInp = get_encInp(batchData,maxQuestLen)\n",
    "# trainImg = get_imgInp(batchData,batchImgId)\n",
    "# #testAns = np.ones((np.asarray(batchDataOp).shape[0],maxAnsLen,vecSize))*wv[\"<Pad>\"]\n",
    "# #testAns[:,0,:] = wv[\"<start>\"]\n",
    "# modelPred = modelQA.predict([trainImg,EncInp])\n",
    "# for i in range(len(batchData)):\n",
    "#     prediction = []\n",
    "#     print(\"Original Question:\"+str(batchData[i]))\n",
    "#     print(\"Original Answer:\"+str(batchDataOp[i]))  \n",
    "#     for p in np.argmax(modelPred[i],1):\n",
    "#         prediction.append(np.asarray(ansVocabdf[\"vocab\"].loc[ansVocabdf[\"index\"]==p])[0])\n",
    "#     print(\"Decoded Answer:\"+str(prediction)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model with actual result analysis \n",
    "#testing on train \n",
    "    \n",
    "def get_ActandPred_list(modelPred,vocab_df,batchDataOp):\n",
    "    PredAnsList = list(list())\n",
    "    actualAnsList= list(list())\n",
    "    for y in batchDataOp:\n",
    "        actualAnsList.append(y[1:])\n",
    "    for i in range(len(batchDataOp)):\n",
    "        prediction = [] \n",
    "        for p in np.argmax(modelPred[i],1):\n",
    "            if p != np.asarray(ansVocabdf[\"index\"].loc[ansVocabdf[\"vocab\"]==\"<Pad>\"]): # <Pad> index in ansVocabdf\n",
    "                prediction.append(np.asarray(ansVocabdf[\"vocab\"].loc[ansVocabdf[\"index\"]==p])[0])\n",
    "        PredAnsList.append(prediction)\n",
    "    return actualAnsList,PredAnsList\n",
    "        \n",
    "    \n",
    "def print_predictions(modelPred,batchData,batchDataOp):\n",
    "    for i in range(len(batchData)):\n",
    "        prediction = []\n",
    "        print(\"Original Question:\"+str(batchData[i]))\n",
    "        print(\"Original Answer:\"+str(batchDataOp[i]))  \n",
    "        for p in np.argmax(modelPred[i],1):\n",
    "            prediction.append(np.asarray(ansVocabdf[\"vocab\"].loc[ansVocabdf[\"index\"]==p])[0])\n",
    "        print(\"Decoded Answer:\"+str(prediction)) \n",
    "        \n",
    "def metric_check(prediction_target,actual):\n",
    "    recall = np.sum((actual + prediction_target)==2)/np.sum(actual==1)\n",
    "    precision = np.sum((actual + prediction_target)==2)/np.sum(prediction_target==1)\n",
    "#     print(\"recall\"+str(np.sum((actual + prediction_target)==2)/np.sum(actual==1))) #recall\n",
    "#     print(\"precision\"+str(np.sum((actual + prediction_target)==2)/np.sum(prediction_target==1)))#precision\n",
    "    return precision, recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiClass_target(ansVocab,vocab_df,batchDataOp):\n",
    "    answerTarget = np.zeros((len(batchDataOp),len(ansVocab)))\n",
    "    for i,a in enumerate(batchDataOp):\n",
    "        for w in a:\n",
    "            answerTarget[i][(vocab_df[\"index\"].loc[vocab_df['vocab'] == w])] = 1\n",
    "    answerTarget[:,vocab_df[\"index\"].loc[vocab_df[\"vocab\"]==\"<start>\"]] = 0\n",
    "    return answerTarget\n",
    "\n",
    "def multiClass_pred(ansVocab,vocab_df,modelPred):\n",
    "    predTarget = np.zeros((len(modelPred),len(ansVocab)))\n",
    "    for i in range(len(modelPred)):  \n",
    "        for p in np.argmax(modelPred[i],1):\n",
    "            predTarget[i][p] = 1 \n",
    "    predTarget[:,vocab_df[\"index\"].loc[vocab_df[\"vocab\"]==\"<Pad>\"]] = 0\n",
    "    return predTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WUPS measure, requires input as list of actual and predicted \n",
    "def wup_measure(a,b,similarity_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Returns Wu-Palmer similarity score.\n",
    "    More specifically, it computes:\n",
    "        max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n",
    "        where interp is a 'interpretation field'\n",
    "    \"\"\"\n",
    "    def get_semantic_field(a):\n",
    "        weight = 1.0\n",
    "        semantic_field = wn.synsets(a,pos=wn.NOUN)\n",
    "        return (semantic_field,weight)\n",
    "\n",
    "\n",
    "    def get_stem_word(a):\n",
    "        \"\"\"\n",
    "        Sometimes answer has form word\\d+:wordid.\n",
    "        If so we return word and downweight\n",
    "        \"\"\"\n",
    "        weight = 1.0\n",
    "        return (a,weight)\n",
    "\n",
    "\n",
    "    global_weight=1.0\n",
    "\n",
    "    (a,global_weight_a)=get_stem_word(a)\n",
    "    (b,global_weight_b)=get_stem_word(b)\n",
    "    global_weight = min(global_weight_a,global_weight_b)\n",
    "\n",
    "    if a==b:\n",
    "        # they are the same\n",
    "        return 1.0*global_weight\n",
    "\n",
    "    if a==[] or b==[]:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    interp_a,weight_a = get_semantic_field(a) \n",
    "    interp_b,weight_b = get_semantic_field(b)\n",
    "\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "\n",
    "    # we take the most optimistic interpretation\n",
    "    global_max=0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score=x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max=local_score\n",
    "\n",
    "    # we need to use the semantic fields and therefore we downweight\n",
    "    # unless the score is high which indicates both are synonyms\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n",
    "    return final_score \n",
    "\n",
    "def fuzzy_set_membership_measure(x,A,m):\n",
    "    return 0 if A==[] else max(map(lambda a: m(x,a), A))\n",
    "\n",
    "def score_it(A,T,m):\n",
    "    if A==[] and T==[]:\n",
    "        return 1\n",
    "    score_left = 1 \n",
    "    score_right = 1 \n",
    "    #for a,p in zip(actual_list,pred_list):\n",
    "    for a1 in A:\n",
    "        score_left= score_left*m(a1,T)\n",
    "    for t1 in T:\n",
    "        score_right= score_right*m(t1,A)\n",
    "    return min(score_left,score_right) \n",
    "\n",
    "our_element_membership=lambda x,y: wup_measure(x,y,0.9)\n",
    "our_set_membership=\\\n",
    "            lambda x,A: fuzzy_set_membership_measure(x,A,our_element_membership)\n",
    "\n",
    "\n",
    "def calculate_WUP(actual_list,pred_list):\n",
    "    score_list = []\n",
    "    if len(actual_list) > 1:\n",
    "        for a,p in zip(actual_list,pred_list):\n",
    "            score_list.append(score_it(a,p,our_set_membership))\n",
    "    final_score=float(sum(score_list))/float(len(score_list))\n",
    "    print('final score is %2.2f%%' % (final_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results on whole dataset\n",
    "# PredAnsList=[]\n",
    "# actualAnsList =[]\n",
    "# totalPrecision =0.0\n",
    "# totalRecall = 0.0\n",
    "# for itr in range(int(math.ceil(np.asarray(questData).shape[0]/batchSize))):\n",
    "#     if (np.asarray(questData).shape[0] - (itr*batchSize) >= batchSize):\n",
    "#         startIx = (itr*batchSize)%(np.asarray(questData).shape[0])\n",
    "#         #startIx = 0\n",
    "#     else:\n",
    "#         break\n",
    "#     # prepare batch data\n",
    "#     batchData = questData[startIx:startIx+batchSize] \n",
    "#     batchDataOp = ansData[startIx:startIx+batchSize]\n",
    "#     batchImgId = imageIds[startIx:startIx+batchSize]\n",
    "#     #encoder Inputs\n",
    "#     trainEncInp = get_encInp(batchData,maxQuestLen)\n",
    "#     #image input\n",
    "#     trainImg = get_imgInp(batchData,batchImgId)\n",
    "#     #answer targets\n",
    "#     #trainTarget = get_oneHotTarget(batchDataOp,len(ansVocab),ansVocabdf)\n",
    "#     modelPred = modelQA.predict([trainImg,trainEncInp])\n",
    "#     target = multiClass_target(ansVocab,ansVocabdf,batchDataOp)\n",
    "#     PredTarget = multiClass_pred(ansVocab,ansVocabdf,modelPred)\n",
    "#     precision,recall = metric_check(PredTarget,target)\n",
    "#     totalPrecision += precision\n",
    "#     totalRecall +=recall\n",
    "#     actualAnsList1,PredAnsList1 = get_ActandPred_list(modelPred,ansVocabdf,batchDataOp)\n",
    "#     actualAnsList.append(actualAnsList1)\n",
    "#     PredAnsList.append(PredAnsList)\n",
    "# calculate_WUP(actualAnsList,PredAnsList)\n",
    "# print(\"Total Precision:\"+ str(np.asarray(totalPrecision).astype(\"float32\")/(itr+1) ))\n",
    "# print(\"Total Recall:\"+ str(np.asarray(totalRecall).astype(\"float32\")/(itr+1) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(modelQA,\"imgTextSimple1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_vaq = load_model(\"imgTextSimple1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.1\n",
      "Recall0.08\n",
      "final score is 10.25%\n",
      "Original Question:['what', 'is', 'the', 'largest', 'white', 'object', 'on', 'the', 'left', 'side', 'of', 'the', 'picture', '?']\n",
      "Original Answer:['<start>', 'printer']\n",
      "Decoded Answer:['shelves', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'between', 'the', 'paper', 'holder', 'and', 'tape', 'dispenser', 'below', 'the', 'white', 'paper', 'rack', '?']\n",
      "Original Answer:['<start>', 'hole', 'puncher']\n",
      "Decoded Answer:['plant', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'the', 'blue', 'object', 'in', 'the', 'black', 'pen', 'stand', '?']\n",
      "Original Answer:['<start>', 'scissor']\n",
      "Decoded Answer:['door', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'color', 'is', 'the', 'fax', 'machine', 'below', 'the', 'paper', 'rack', 'in', 'front', 'of', 'the', 'wall', 'on', 'the', 'top', 'of', 'the', 'cabinet', '?']\n",
      "Original Answer:['<start>', 'white']\n",
      "Decoded Answer:['white', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['how', 'many', 'blue', 'objects', 'are', 'on', 'the', 'top', 'of', 'the', 'cabinet', 'but', 'not', 'below', 'the', 'tape', 'dispenser', 'or', 'in', 'the', 'pen', 'stand', '?']\n",
      "Original Answer:['<start>', '1']\n",
      "Decoded Answer:['5', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'the', 'black', 'object', 'between', 'the', 'white', 'wall', 'and', 'gray', 'locker', '?']\n",
      "Original Answer:['<start>', 'garbage', 'bin']\n",
      "Decoded Answer:['door', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'on', 'the', 'right', 'side', 'of', 'the', 'wall', 'on', 'the', 'top', 'of', 'the', 'cabinet', 'on', 'the', 'left', 'side', 'of', 'the', 'hole', 'puncher', '?']\n",
      "Original Answer:['<start>', 'paper', 'holder']\n",
      "Decoded Answer:['paper', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'color', 'is', 'the', 'tape', 'dispenser', 'on', 'the', 'right', 'side', 'of', 'the', 'hole', 'puncher', 'in', 'front', 'of', 'the', 'wall', '?']\n",
      "Original Answer:['<start>', 'gray']\n",
      "Decoded Answer:['red', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['how', 'many', 'table', 'fans', 'are', 'in', 'this', 'picture', '?']\n",
      "Original Answer:['<start>', '1']\n",
      "Decoded Answer:['5', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'the', 'black', 'and', 'white', 'object', 'on', 'the', 'cabinet', 'door', 'between', 'the', 'pictures', '?']\n",
      "Original Answer:['<start>', 'clock']\n",
      "Decoded Answer:['globe', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['how', 'many', 'chairs', 'are', 'around', 'the', 'conference', 'table', '?']\n",
      "Original Answer:['<start>', '6']\n",
      "Decoded Answer:['5', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'has', 'been', 'mounted', 'on', 'the', 'ceiling', 'between', 'two', 'ceiling', 'lights', '?']\n",
      "Original Answer:['<start>', 'projector']\n",
      "Decoded Answer:['photo', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'color', 'is', 'the', 'chair', 'in', 'front', 'of', 'the', 'wall', 'on', 'the', 'right', 'side', 'of', 'the', 'door', 'behind', 'the', 'black', 'chair', '?']\n",
      "Original Answer:['<start>', 'pink']\n",
      "Decoded Answer:['red', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'on', 'the', 'top', 'of', 'the', 'computer', 'case', 'on', 'the', 'right', 'side', 'of', 'the', 'picture', '?']\n",
      "Original Answer:['<start>', 'modem']\n",
      "Decoded Answer:['monitor', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'color', 'is', 'the', 'table', 'below', 'the', 'computer', 'case', 'in', 'front', 'of', 'the', 'wall', '?']\n",
      "Original Answer:['<start>', 'brown']\n",
      "Decoded Answer:['red', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['how', 'many', 'papers', 'are', 'on', 'the', 'wall', 'on', 'the', 'left', 'side', 'of', 'the', 'modem', '?']\n",
      "Original Answer:['<start>', '1']\n",
      "Decoded Answer:['5', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'the', 'brown', 'object', 'on', 'the', 'desk', 'on', 'the', 'left', 'side', 'of', 'the', 'paper', 'in', 'fornt', 'of', 'the', 'computer', 'chair', '?']\n",
      "Original Answer:['<start>', 'paper', 'tray']\n",
      "Decoded Answer:['telephone', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'color', 'is', 'the', 'garbage', 'bin', 'on', 'the', 'floor', 'on', 'the', 'left', 'side', 'of', 'the', 'desk', '?']\n",
      "Original Answer:['<start>', 'purple']\n",
      "Decoded Answer:['clock', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'is', 'between', 'the', 'computer', 'chair', 'and', 'sofa', 'and', 'on', 'the', 'right', 'side', 'of', 'the', 'column', '?']\n",
      "Original Answer:['<start>', 'wall', 'divider']\n",
      "Decoded Answer:['decorative', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n",
      "Original Question:['what', 'color', 'is', 'the', 'paper', 'holder', 'on', 'the', 'right', 'side', 'of', 'the', 'water', 'spray', 'bottle', '?']\n",
      "Original Answer:['<start>', 'black']\n",
      "Decoded Answer:['telephone', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>', '<Pad>']\n"
     ]
    }
   ],
   "source": [
    "modelPred = loaded_model_vaq.predict([trainImg,EncInp])\n",
    "target = multiClass_target(ansVocab,ansVocabdf,batchDataOp)\n",
    "PredTarget = multiClass_pred(ansVocab,ansVocabdf,modelPred)\n",
    "precision,recall = metric_check(PredTarget,target)\n",
    "actualAnsList,PredAnsList = get_ActandPred_list(modelPred,ansVocabdf,batchDataOp)\n",
    "print(\"Precision:\"+str(precision))\n",
    "print(\"Recall\"+str(recall))\n",
    "calculate_WUP(actualAnsList,PredAnsList)\n",
    "print_predictions(modelPred,batchData,batchDataOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add test on complete data\n",
    "#save model and results \n",
    "#experiment with regularization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
